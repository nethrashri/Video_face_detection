{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85056b-f9b2-497b-89e8-118d4d53b510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless fer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a553f-a4ea-4c5d-80d6-8acaeccb1ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import os\n",
    "from collections import Counter\n",
    "from IPython.display import display, Image\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "# Initialize FER emotion detector\n",
    "emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "def analyze_images_emotions(image_paths):\n",
    "    emotion_sums = {\n",
    "        'angry': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'happy': 0,\n",
    "        'sad': 0,\n",
    "        'surprise': 0,\n",
    "        'neutral': 0\n",
    "    }\n",
    "    all_emotions = []\n",
    "    frame_count = 0\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Processing image file: {image_path}\")\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Error: Unable to open image file {image_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Analyze the emotions in the image\n",
    "        emotions = emotion_detector.detect_emotions(image)\n",
    "        if emotions:\n",
    "            dominant_emotion = max(emotions[0]['emotions'], key=emotions[0]['emotions'].get)\n",
    "            all_emotions.append(dominant_emotion)\n",
    "            for emotion, score in emotions[0]['emotions'].items():\n",
    "                emotion_sums[emotion] += score\n",
    "            frame_count += 1\n",
    "\n",
    "            # Annotate the image with the detected emotions\n",
    "            (x, y, w, h) = emotions[0][\"box\"]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(image, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "            # Convert the image to RGB (from BGR) for display in Jupyter\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Convert the image to a format that can be displayed in Jupyter\n",
    "            pil_img = PIL.Image.fromarray(image_rgb)\n",
    "            buffer = io.BytesIO()\n",
    "            pil_img.save(buffer, format='PNG')\n",
    "            display(Image(data=buffer.getvalue()))\n",
    "\n",
    "            # Print the detected emotion\n",
    "            print(f\"Dominant emotion in {image_path}: {dominant_emotion}\")\n",
    "        else:\n",
    "            print(f\"No emotions detected in {image_path}\")\n",
    "\n",
    "    if frame_count == 0:\n",
    "        print(\"No images were processed.\")\n",
    "        return None, None\n",
    "\n",
    "    # Calculate average emotions\n",
    "    average_emotions = {emotion: score / frame_count for emotion, score in emotion_sums.items()}\n",
    "\n",
    "    # Determine the predominant emotion\n",
    "    emotion_counts = Counter(all_emotions)\n",
    "    predominant_emotion = emotion_counts.most_common(1)[0][0] if all_emotions else None\n",
    "\n",
    "    print(f\"Processed {frame_count} images.\")\n",
    "    return predominant_emotion, average_emotions\n",
    "\n",
    "# List of image file paths\n",
    "image_folder_path = 'images'\n",
    "image_paths = [os.path.join(image_folder_path, img) for img in os.listdir(image_folder_path) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Analyze the images\n",
    "result = analyze_images_emotions(image_paths)\n",
    "\n",
    "if result:\n",
    "    predominant_emotion, average_emotions = result\n",
    "    if predominant_emotion:\n",
    "        print(f\"Predominant emotion: {predominant_emotion}\")\n",
    "    else:\n",
    "        print(\"No predominant emotion detected.\")\n",
    "    if average_emotions:\n",
    "        print(\"Average emotions:\")\n",
    "        for emotion, score in average_emotions.items():\n",
    "            print(f\"{emotion}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"No images were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67200c3-f764-49ee-8771-9127cc6b0404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3be75-960b-48ee-bbc8-6a50118046ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import os\n",
    "from collections import Counter\n",
    "from IPython.display import display, Image\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "# Initialize FER emotion detector\n",
    "emotion_detector = FER(mtcnn=True)\n",
    "\n",
    "def analyze_images_emotions(image_paths):\n",
    "    emotion_sums = {\n",
    "        'angry': 0,\n",
    "        'disgust': 0,\n",
    "        'fear': 0,\n",
    "        'happy': 0,\n",
    "        'sad': 0,\n",
    "        'surprise': 0,\n",
    "        'neutral': 0\n",
    "    }\n",
    "    all_emotions = []\n",
    "    frame_count = 0\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Processing image file: {image_path}\")\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is None:\n",
    "            print(f\"Error: Unable to open image file {image_path}.\")\n",
    "            continue\n",
    "\n",
    "        # Resize the image to a consistent size for analysis\n",
    "        # Keeping the aspect ratio can be important, but you may decide on a standard size\n",
    "        height, width = image.shape[:2]\n",
    "        aspect_ratio = width / height\n",
    "        new_height = 480\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        image_resized = cv2.resize(image, (new_width, new_height))\n",
    "\n",
    "        # Analyze the emotions in the image\n",
    "        emotions = emotion_detector.detect_emotions(image_resized)\n",
    "        if emotions:\n",
    "            dominant_emotion = max(emotions[0]['emotions'], key=emotions[0]['emotions'].get)\n",
    "            all_emotions.append(dominant_emotion)\n",
    "            for emotion, score in emotions[0]['emotions'].items():\n",
    "                emotion_sums[emotion] += score\n",
    "            frame_count += 1\n",
    "\n",
    "            # Annotate the image with the detected emotions\n",
    "            (x, y, w, h) = emotions[0][\"box\"]\n",
    "            cv2.rectangle(image_resized, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(image_resized, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "            # Convert the image to RGB (from BGR) for display in Jupyter\n",
    "            image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Convert the image to a format that can be displayed in Jupyter\n",
    "            pil_img = PIL.Image.fromarray(image_rgb)\n",
    "            buffer = io.BytesIO()\n",
    "            pil_img.save(buffer, format='PNG')\n",
    "            display(Image(data=buffer.getvalue()))\n",
    "\n",
    "            # Print the detected emotion\n",
    "            print(f\"Dominant emotion in {image_path}: {dominant_emotion}\")\n",
    "        else:\n",
    "            print(f\"No emotions detected in {image_path}\")\n",
    "\n",
    "    if frame_count == 0:\n",
    "        print(\"No images were processed.\")\n",
    "        return None, None\n",
    "\n",
    "    # Calculate average emotions\n",
    "    average_emotions = {emotion: score / frame_count for emotion, score in emotion_sums.items()}\n",
    "\n",
    "    # Determine the predominant emotion\n",
    "    emotion_counts = Counter(all_emotions)\n",
    "    predominant_emotion = emotion_counts.most_common(1)[0][0] if all_emotions else None\n",
    "\n",
    "    print(f\"Processed {frame_count} images.\")\n",
    "    return predominant_emotion, average_emotions\n",
    "\n",
    "# List of image file paths\n",
    "image_folder_path = 'images'\n",
    "image_paths = [os.path.join(image_folder_path, img) for img in os.listdir(image_folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.gif','.avif'))]\n",
    "\n",
    "# Analyze the images\n",
    "result = analyze_images_emotions(image_paths)\n",
    "\n",
    "if result:\n",
    "    predominant_emotion, average_emotions = result\n",
    "    if predominant_emotion:\n",
    "        print(f\"Predominant emotion: {predominant_emotion}\")\n",
    "    else:\n",
    "        print(\"No predominant emotion detected.\")\n",
    "    if average_emotions:\n",
    "        print(\"Average emotions:\")\n",
    "        for emotion, score in average_emotions.items():\n",
    "            print(f\"{emotion}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"No images were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc45e2-fb2a-4ccb-8eaf-7c0f6957ebcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
